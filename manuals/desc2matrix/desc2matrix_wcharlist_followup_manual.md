# desc2matrix_wcharlist_followup.py user manual

This document explains the up-to-date manual for using `desc2matrix_wcharlist_followup.py`, which converts the description text files generated by the `Makefile` into a structured JSON output containing the traits and corresponding values. The difference between `desc2matrix_py` and this script is that this includes a pre-determined list of traits in the prompt so that model outputs are more or less standardised across species. Additionally, this script is different from `desc2matrix_wcharlist.py` in that it asks the LLM a 'follow-up' question after every initial prompt including a list of words that it has omitted from the original description, in order to make the output more complete.

## Operation

1. Retrieve the initial list of traits from the charlistfile

2. Retrieve trait from a species using the list of trait names, using the initial 'extraction prompt'

The names of traits are fed into the LLM run extracting the traits of the subsequent species. The model is asked to extract the given list of traits, put 'NA' where a given trait is missing in the description, and add any traits that are not in the list but are mentioned in the species description.

3. For the species, ask a follow-up question including a list of non-stop words in the original description that the LLM has omitted in its response

The follow-up prompt can be seen below.

4. Repeat steps 2 and 3 for every species

## Default prompts

The default prompts are hard-coded in `common_scripts/default_prompts.py`, but can be imported from a text file using the relevant options (see **Arguments**). The prompt must include the following special 'markers':

| Marker | In: | Meaning |
| --- | --- | --- |
| `[DESCRIPTION]` | Extraction prompt, Follow-up prompt | Plant description text to compile |
| `[CHARACTER_LIST]` | Extraction prompt, Follow-up prompt | Given list of traits to extract |
| `[MISSING_WORDS]` | Follow-up prompt | List of non-stop words in the plant description that were initially omitted |

### System prompt

See `global_sys_prompt` in `common_scripts/default_prompts.py`.

### Extraction prompt

See `global_prompt` in `common_scripts/default_prompts.py`.

### Follow-up prompt

See `global_followup_prompt` in `common_scripts/default_prompts.py`.

## Arguments

| Argument | Description | Required? | Default value |
| --- | --- | --- | --- |
| `descfile` (positional argument) | Path to the flora description file to transcribe | Yes | |
| `charlistfile` (positional argument) | Path to the list of traits to use | Yes | |
| `--charlistsep` | Character or string to use as separator in `charlistfile` | No | `,` |
| `outputfile` (positional argument) | Path to the output JSON file | Yes | |
| `--desctype` | Name of the 'type' that contains morphological descriptions in the descfile | Yes | |
| `--sysprompt` | Path to a text file containing the system prompt to use | No | See above |
| `--prompt` | Path to a text file containing the prompt to use | No | See above |
| `--fprompt` | Path to a text file containing the follow-up prompt to use | No | See above |
| `--silent` | If flag is present, suppress command-line output showing progress | No | `None` |
| `--start` | Order ID (starting from 0) of the species in the descfile to start transcribing from | No | `0` |
| `--spnum` | Number of species to transcribe | No | `None` (transcribe entire file) |
| `--model` | Name of the base LLM to use. Specified LLM must be installed and running at `localhost:11434` | No | `llama3` |
| `--temperature` | Model temperature between 0 and 1. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `0.1` |
| `--seed` | Random seed to use for reproducibility. Setting to 0 makes the output random. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `1` |
| `--repeatlastn` | Number of tokens(?) the model looks back to prevent repetition. Set to 0 to prevent this behaviour as default. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | 0 |
| `--numpredict` | Number of tokens for model to generate. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `2048` |
| `--numctx` | Size of the context window used to generate the token See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `4096` |
| `--topk` | Parameter adjusting the degree of 'conservativeness' in the model output. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `None` (set to `40` by Ollama) |
| `--topp` | Parameter adjusting the degree of 'conservativeness' in the model output. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `None` (set to `0.9` by Ollama) |

## Output

The output is a single JSON object with the following keys:

| Key | Description |
| --- | --- |
| `metadata` | Run metadata |
| `data` | List of JSONs containing transcribed characteristics of each species (see below) |

The `metadata` is itself a JSON containing the following keys:

| Key | Description |
| --- | --- |
| `sys_prompt` | System prompt |
| `prompt` | Prompt used subsequent to `init_prompt` to extract the characteristics |
| `params` | Key-value pairs of parameters used for the model run |
| `mode` | 'Mode' used to generate the output. This is set to `desc2json_wcharlist_followup`. |
| `charlist` | A list of characteristics provided in the prompt for trait extraction |

The `data` list contains JSON objects for every transcribed species, with the following keys:

| Key | Description |
| --- | --- |
| `coreid` | WFO taxon ID of the transcribed species |
| `status` | `success` for successful parse, `invalid_json` for invalid JSON output, `bad_structure` for JSON that's valid but badly structured. The unsuccessful parse statuses can also have `_followup` at the end, which indicates that the error has ocurred in the response to the follow-up prompt. |
| `original_description` | The original description imported from WFO |
| `char_json` | List of JSONS containing the transcribed characteristics. Each element is structured as `{"characteristic": name of characteristic, "value": value of characteristic}`. This is `null` if the response failed to parse. |
| `failed_str` | Response string from the LLM that failed to parse to JSON. This is `null` if the response successfully parsed. |