# desc2matrix_accum.py user manual

This document explains the up-to-date manual for using `desc2matrix_accum.py`, which converts the description text files generated by the `Makefile` into a structured JSON output containing the traits and corresponding values. The difference between `desc2matrix_py` and this script is that this accumulates the list of the names of traits as it processes the species descriptions.

## Operation

1. Retrieve the initial list of traits from the first species description in the provided descfile

The model runs the 'intial prompt', which is the same prompt as `desc2matrix.py`, on the first description in the descfile and retrieves the JSON of traits in the same manner. Then, the names of the traits are isolated.

2. Retrieve traits from the rest of the species using a growing list of trait names

The names of traits are then fed into the LLM run extracting the traits of the subsequent species. The model is asked to extract the given list of traits, put 'NA' where a given trait is missing in the description, and add any traits that are not in the list but are mentioned in the species description.

3. Update the list of traits if needed

The running list of traits is **only updated if the new list of traits is longer than the previous list**. This is to prevent the list from 'degrading' as the script processes the species. If a particular run fails due to the LLM returning a badly structured output, the list of traits is kept the same for the next species.

## Default prompts

The default prompts are hard-coded in `common_scripts/default_prompts.py`, but can be imported from a text file using the relevant options (see **Arguments**). The initial prompt and the general extraction prompt must include one or more of the following special 'markers', as is appropriate:

| Marker | In: | Meaning |
| --- | --- | --- |
| `[DESCRIPTION]` | Initial prompt, Extraction prompt | Plant description text to compile |
| `[CHARACTER_LIST]` | Extraction prompt | Given list of traits to extract |

### System prompt

See `global_sys_prompt` in `common_scripts/default_prompts.py`.

### Extraction prompt

See `global_prompt` in `common_scripts/default_prompts.py`.

## Arguments

| Argument | Description | Required? | Default value |
| --- | --- | --- | --- |
| `descfile` (positional argument) | Path to the flora description file to transcribe | Yes | |
| `outputfile` (positional argument) | Path to the output JSON file | Yes | |
| `--desctype` | Name of the 'type' that contains morphological descriptions in the descfile | Yes | |
| `--sysprompt` | Path to a text file containing the system prompt to use | No | See above |
| `--prompt` | Path to a text file containing the prompt to use | No | See above |
| `--initprompt` | Path to a text file containing the initial prompt to use | No | See above |
| `--silent` | If flag is present, suppress command-line output showing progress | No | `None` |
| `--start` | Order ID (starting from 0) of the species in the descfile to start transcribing from | No | `0` |
| `--spnum` | Number of species to transcribe | No | `None` (transcribe entire file) |
| `--model` | Name of the base LLM to use. Specified LLM must be installed and running at `localhost:11434` | No | `llama3` |
| `--temperature` | Model temperature between 0 and 1. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `0.1` |
| `--seed` | Random seed to use for reproducibility. Setting to 0 makes the output random. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `1` |
| `--repeatlastn` | Number of tokens(?) the model looks back to prevent repetition. Set to 0 to prevent this behaviour as default. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | 0 |
| `--numpredict` | Number of tokens for model to generate. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `2048` |
| `--numctx` | Size of the context window used to generate the token See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `4096` |
| `--topk` | Parameter adjusting the degree of 'conservativeness' in the model output. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `None` (set to `40` by Ollama) |
| `--topp` | Parameter adjusting the degree of 'conservativeness' in the model output. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `None` (set to `0.9` by Ollama) |

## Output

The output is a single JSON object with the following keys:

| Key | Description |
| --- | --- |
| `metadata` | Run metadata |
| `data` | List of JSONs containing transcribed characteristics of each species (see below) |
| `charlist_len_history` | A list containing the length of the lists of characteristics used for each species processed |
| `charlist_history` | A list containing the lists of characteristics used for each species processed |

The `metadata` is itself a JSON containing the following keys:

| Key | Description |
| --- | --- |
| `sys_prompt` | System prompt |
| `init_prompt` | Prompt used to get the initial list of characteristics |
| `prompt` | Prompt used subsequent to `init_prompt` to extract the characteristics |
| `params` | Key-value pairs of parameters used for the model run |
| `mode` | 'Mode' used to generate the output. This is set to `desc2json_accum`. |

The `data` list contains JSON objects for every transcribed species, with the following keys:

| Key | Description |
| --- | --- |
| `coreid` | WFO taxon ID of the transcribed species |
| `status` | `success` for successful parse, `invalid_json` for invalid JSON output, `bad_structure` for JSON that's valid but badly structured. |
| `original_description` | The original description imported from WFO |
| `char_json` | List of JSONS containing the transcribed characteristics. Each element is structured as `{"characteristic": name of characteristic, "value": value of characteristic}`. This is `null` if the response failed to parse. |
| `failed_str` | Response string from the LLM that failed to parse to JSON. This is `null` if the response successfully parsed. |